{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b56b6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 20 Newsgroups Dataset Preprocessing Tutorial\n",
    "\n",
    "# This notebook demonstrates how to preprocess the 20 Newsgroups dataset for NLP tasks. We'll walk through each step of cleaning and preparing the text data, including:\n",
    "\n",
    "# 1. Loading and parsing the raw text files\n",
    "# 2. Cleaning the text (removing punctuation, numbers, emails, URLs)\n",
    "# 3. Tokenization\n",
    "# 4. Stopword removal\n",
    "# 5. Lemmatization\n",
    "# 6. Creating a final clean DataFrame\n",
    "\n",
    "# The output will be a CSV file containing processed text data ready for NLP tasks.\n",
    "\n",
    "# ## Setup and Dependencies\n",
    "\n",
    "# First, let's install and import the required packages. We'll use:\n",
    "# - pandas: for data manipulation\n",
    "# - nltk: for text processing\n",
    "# - spacy: for advanced NLP tasks\n",
    "# - re: for regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b41a7cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading required NLTK data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msabdelnasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msabdelnasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\msabdelnasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\msabdelnasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK downloads complete!\n",
      "Punkt tokenizer is available!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading required NLTK data...\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "print(\"NLTK downloads complete!\")\n",
    "\n",
    "# Verify punkt tokenizer is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"Punkt tokenizer is available!\")\n",
    "except LookupError:\n",
    "    print(\"Retrying Punkt download...\")\n",
    "    nltk.download('punkt', quiet=False)\n",
    "\n",
    "# Load English language model for spaCy\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy English model...\")\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Import specific NLTK modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer and get stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6b13d",
   "metadata": {},
   "source": [
    "## Data Loading and Parsing\n",
    "\n",
    "Now we'll load and parse the dataset. The data is organized as follows:\n",
    "1. A `list.csv` file mapping document IDs to newsgroups\n",
    "2. 20 text files, each containing multiple documents\n",
    "3. Each document has a header with newsgroup, ID, author, and subject\n",
    "4. The body text follows until the next document header\n",
    "\n",
    "Let's start by reading the list.csv file and creating functions to parse the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20bb6da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 628\n",
      "\n",
      "Sample of list.csv:\n",
      "            newsgroup  document_id\n",
      "0  talk.religion.misc        82757\n",
      "1  talk.religion.misc        82758\n",
      "2  talk.religion.misc        82759\n",
      "3  talk.religion.misc        82760\n",
      "4  talk.religion.misc        82763\n"
     ]
    }
   ],
   "source": [
    "# Read the document ID to newsgroup mapping\n",
    "df_list = pd.read_csv(r'C:\\Projects\\Training\\AI-project-structured-code-template\\NLP\\data\\list.csv')\n",
    "print(\"Number of documents:\", len(df_list))\n",
    "print(\"\\nSample of list.csv:\")\n",
    "print(df_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93ead62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def read_file(file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Read the file and return its contents as a list of lines.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the newsgroup file\n",
    "\n",
    "    Returns:\n",
    "        List of strings, each string being a line from the file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def is_header_start(line: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a line is the start of a message (starts with 'Newsgroup:').\n",
    "\n",
    "    Args:\n",
    "        line: The line to check\n",
    "\n",
    "    Returns:\n",
    "        bool: True if this is a message header start\n",
    "    \"\"\"\n",
    "    return line.strip().startswith('Newsgroup:')\n",
    "\n",
    "def get_newsgroup_from_path(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract newsgroup name from the file path.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the newsgroup file\n",
    "\n",
    "    Returns:\n",
    "        str: Name of the newsgroup\n",
    "    \"\"\"\n",
    "    return Path(file_path).stem\n",
    "\n",
    "def extract_messages(lines: List[str], newsgroup: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract individual messages from the file content.\n",
    "\n",
    "    Args:\n",
    "        lines: List of lines from the file\n",
    "        newsgroup: Name of the newsgroup\n",
    "\n",
    "    Returns:\n",
    "        List of tuples (newsgroup, message_body)\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    current_message = []\n",
    "    in_message = False\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        current_line = lines[i].rstrip()  # Remove trailing whitespace\n",
    "\n",
    "        # Check if this is a new message header\n",
    "        if is_header_start(current_line):\n",
    "            if in_message and current_message:\n",
    "                # Save the previous message\n",
    "                messages.append((newsgroup, ''.join(current_message)))\n",
    "                current_message = []\n",
    "            in_message = True\n",
    "            current_message = [current_line + '\\n']  # Start new message\n",
    "        elif in_message:\n",
    "            current_message.append(current_line + '\\n')\n",
    "\n",
    "    # Don't forget the last message\n",
    "    if current_message:\n",
    "        messages.append((newsgroup, ''.join(current_message)))\n",
    "\n",
    "    return messages\n",
    "\n",
    "def parse_document(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a newsgroup file and return a DataFrame with newsgroup and body columns.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the newsgroup file\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame with columns ['newsgroup', 'body']\n",
    "    \"\"\"\n",
    "    # Get newsgroup name from file path\n",
    "    newsgroup = get_newsgroup_from_path(file_path)\n",
    "\n",
    "    # Read the file\n",
    "    lines = read_file(file_path)\n",
    "\n",
    "    # Extract messages\n",
    "    messages = extract_messages(lines, newsgroup)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(messages, columns=['newsgroup', 'body'])\n",
    "\n",
    "    # Display some statistics\n",
    "    print(f\"File: {Path(file_path).name}\")\n",
    "    print(f\"Total messages found: {len(df)}\")\n",
    "    if len(df) > 0:\n",
    "        print(\"\\nFirst message preview:\")\n",
    "        print(\"Newsgroup:\", df.iloc[0]['newsgroup'])\n",
    "        print(\"Message start (first 200 chars):\", df.iloc[0]['body'][:200])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74e541",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "\n",
    "Now we'll implement text cleaning functions to:\n",
    "1. Convert text to lowercase\n",
    "2. Remove email addresses\n",
    "3. Remove URLs\n",
    "4. Remove punctuation and numbers\n",
    "5. Remove extra whitespace\n",
    "\n",
    "We'll create a pipeline of cleaning functions that can be applied to both subject and body text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45df64ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      "\n",
      "Original body (first 200 chars): From: mathew <mathew@mantis.co.uk>\n",
      "Subject: Alt.Atheism FAQ: Atheist Resources\n",
      "\n",
      "Archive-name: atheism/resources\n",
      "Alt-atheism-archive-name: resources\n",
      "Last-modified: 11 December 1992\n",
      "Version: 1.0\n",
      "\n",
      "      \n",
      "Cleaned body (first 200 chars): from mathew subject alt atheism faq atheist resources archive name atheism resources alt atheism archive name resources last modified december version atheist resources addresses of atheist organizati\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Apply all cleaning steps to the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning to body text\n",
    "df['body_clean'] = df['body'].apply(clean_text)\n",
    "\n",
    "print(\"Sample cleaned text:\")\n",
    "print(\"\\nOriginal body (first 200 chars):\", df.iloc[0]['body'][:200])\n",
    "print(\"Cleaned body (first 200 chars):\", df.iloc[0]['body_clean'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01661576",
   "metadata": {},
   "source": [
    "## Tokenization and Stopword Removal\n",
    "\n",
    "Now we'll tokenize the cleaned text and remove stopwords. We'll:\n",
    "1. Split text into individual tokens\n",
    "2. Remove stopwords\n",
    "3. Store the tokens in new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6ea9f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\msabdelnasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "69768526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenization:\n",
      "\n",
      "Cleaned body (first 100 chars): from mathew subject alt atheism faq atheist resources archive name atheism resources alt atheism arc\n",
      "First 20 tokens: ['mathew', 'subject', 'alt', 'atheism', 'faq', 'atheist', 'resources', 'archive', 'name', 'atheism', 'resources', 'alt', 'atheism', 'archive', 'name', 'resources', 'last', 'modified', 'december', 'version']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_remove_stopwords(text):\n",
    "    \"\"\"Tokenize text and remove stopwords using basic string splitting.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # Simple tokenization by splitting on whitespace\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to cleaned text\n",
    "df['body_tokens'] = df['body_clean'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "print(\"Sample tokenization:\")\n",
    "print(\"\\nCleaned body (first 100 chars):\", df.iloc[0]['body_clean'][:100])\n",
    "print(\"First 20 tokens:\", df.iloc[0]['body_tokens'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7461064b",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Now we'll apply lemmatization to reduce words to their base form. For example:\n",
    "- \"running\" → \"run\"\n",
    "- \"better\" → \"good\"\n",
    "- \"was\" → \"be\"\n",
    "\n",
    "We'll use NLTK's WordNetLemmatizer, which provides more accurate results than simple stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "55f7e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization example:\n",
      "\n",
      "First 20 original tokens: ['mathew', 'subject', 'alt', 'atheism', 'faq', 'atheist', 'resources', 'archive', 'name', 'atheism', 'resources', 'alt', 'atheism', 'archive', 'name', 'resources', 'last', 'modified', 'december', 'version']\n",
      "First 20 lemmatized tokens: ['mathew', 'subject', 'alt', 'atheism', 'faq', 'atheist', 'resource', 'archive', 'name', 'atheism', 'resource', 'alt', 'atheism', 'archive', 'name', 'resource', 'last', 'modified', 'december', 'version']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Apply lemmatization to a list of tokens.\"\"\"\n",
    "    if not isinstance(tokens, list):\n",
    "        return []\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply lemmatization\n",
    "df['body_lemmas'] = df['body_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Show example\n",
    "print(\"Lemmatization example:\")\n",
    "print(\"\\nFirst 20 original tokens:\", df.iloc[0]['body_tokens'][:20])\n",
    "print(\"First 20 lemmatized tokens:\", df.iloc[0]['body_lemmas'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d53b5",
   "metadata": {},
   "source": [
    "## Create Final Dataset\n",
    "\n",
    "Now we'll prepare the final dataset with the columns we want to keep:\n",
    "- document_id\n",
    "- newsgroup\n",
    "- subject\n",
    "- body_clean\n",
    "- tokens (from body)\n",
    "\n",
    "We'll join the tokens back into strings for easier handling in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4b1ad94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset info:\n",
      "\n",
      "Shape: (37662, 3)\n",
      "\n",
      "Columns: ['newsgroup', 'body_clean', 'tokens']\n",
      "\n",
      "Sample row:\n",
      "newsgroup                                           alt.atheism\n",
      "body_clean    from mathew subject alt atheism faq atheist re...\n",
      "tokens        mathew subject alt atheism faq atheist resourc...\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Dataset saved to preprocessed_news.csv\n",
      "\n",
      "First few rows of the saved dataset:\n",
      "\n",
      "Dataset saved to preprocessed_news.csv\n",
      "\n",
      "First few rows of the saved dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newsgroup</th>\n",
       "      <th>body_clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>from mathew subject alt atheism faq atheist re...</td>\n",
       "      <td>mathew subject alt atheism faq atheist resourc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>alt atheism document_id from mathew subject al...</td>\n",
       "      <td>alt atheism document_id mathew subject alt ath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>alt atheism document_id from mathew subject al...</td>\n",
       "      <td>alt atheism document_id mathew subject alt ath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>alt atheism document_id from benedikt rosenau ...</td>\n",
       "      <td>alt atheism document_id benedikt rosenau subje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>alt atheism document_id from mathew subject re...</td>\n",
       "      <td>alt atheism document_id mathew subject univers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     newsgroup                                         body_clean  \\\n",
       "0  alt.atheism  from mathew subject alt atheism faq atheist re...   \n",
       "1  alt.atheism  alt atheism document_id from mathew subject al...   \n",
       "2  alt.atheism  alt atheism document_id from mathew subject al...   \n",
       "3  alt.atheism  alt atheism document_id from benedikt rosenau ...   \n",
       "4  alt.atheism  alt atheism document_id from mathew subject re...   \n",
       "\n",
       "                                              tokens  \n",
       "0  mathew subject alt atheism faq atheist resourc...  \n",
       "1  alt atheism document_id mathew subject alt ath...  \n",
       "2  alt atheism document_id mathew subject alt ath...  \n",
       "3  alt atheism document_id benedikt rosenau subje...  \n",
       "4  alt atheism document_id mathew subject univers...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final DataFrame with selected columns\n",
    "final_df = df[['newsgroup', 'body_clean']].copy()\n",
    "\n",
    "# Add tokens column (joined into string for CSV storage)\n",
    "final_df['tokens'] = df['body_lemmas'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "# Display info about the final dataset\n",
    "print(\"Final dataset info:\")\n",
    "print(\"\\nShape:\", final_df.shape)\n",
    "print(\"\\nColumns:\", final_df.columns.tolist())\n",
    "print(\"\\nSample row:\")\n",
    "print(final_df.iloc[0])\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'preprocessed_news.csv'\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nDataset saved to {output_path}\")\n",
    "\n",
    "# Display first few rows of the saved dataset\n",
    "print(\"\\nFirst few rows of the saved dataset:\")\n",
    "pd.read_csv(output_path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d082bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
